{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fallout76 Case Study Webscraping and Sentiment Analysis Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our LIS3201 semester-long research project, my team and I investigated toxic reactions in video game journalism headlines, specifically headlines related to the video game Fallout76.\n",
    "\n",
    "In order to accomplish this I developed a python script that allowed us to scrape Google news headlines about Fallout 76 and then analyze their sentiment index to get quantitative data on toxic reactions. Below you will find the complete code of my first working script along with a walkthrough of comments explaining each step of the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to any script is to import all the necessary libraries. Here is a list of the libraries we imported and what they do:\n",
    "* requests - access webpages\n",
    "* bs4 - BeautifulSoup4, takes raw HTML from webpages and turns it into objects in python that can be manipulated easily\n",
    "* nltk - Natural Language Toolkit, leading python library for working with human language data. We used it for tokenization, the sentiment analyzer and its stopwords list (all these will be explained later).\n",
    "* csv - exporting data as Comma Seperated Value files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Requesting the Webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before I request the webpage, I set up a `headers` variable and a `payload` variable that I could use to request the correct webpage. The `headers` variable contains information about you as the requester so that the page knows who you are and what browser you're using. I found that unless I used this specific header, Google would give me the wrong webpage and just generally not cooperate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\":\n",
    "        \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `payload` variable contains information about the query you're making. In my case it included the search term, the window of time we wanted to look for results in, and the specific subset of Google News results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'as_epq': 'Fallout 76', 'tbs':'cdr:1,cd_min:11/14/2018,cd_max:12/14/2018', 'tbm':'nws'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have my `headers` and `payload`, I can request the webpage and save the response as an object called `r`. We used the `.get` method to go out and scrape the page. This method takes three arguments, first the actual base url of the site you're going to, in my case Google, and then `params` which is just our `payload` variable and `headers` which is our `headers` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.google.com/search\", params=payload, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I put a print statement just to verify that the request went to the right page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL =  https://www.google.com/search?as_epq=Fallout+76&tbs=cdr%3A1%2Ccd_min%3A11%2F14%2F2018%2Ccd_max%3A12%2F14%2F2018&tbm=nws\n"
     ]
    }
   ],
   "source": [
    "print(\"URL = \", r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Getting the Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that I've requested a webpage and have it saved as the `r` object, I can start to figure out where our headlines are and pull them out of the raw HTML code. The first step here is to bring in the BeautifulSoup library to make the `r` object easier to work with. I created a new object called `soup` and then used BeautifulSoup to select only the text of r and in an lxml format so I can manipulate it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still contains all the HTML from the website, most of which is totally irrelevant to what I'm doing. So I used the `select()` method to go in and select only the elements I wanted from the whole page. `select()` takes CSS selectors as its arguments, so I put `div.g.card h3` in because all the headlines are in h3 tags within divs that have the class of g and card. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = soup.select(\"div.g.card h3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `base` only includes the headlines but they still have HTML elements wrapped around them, which I need to get rid of. Below is a print statement to illustrate what one looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW HEADLINES= <h3 class=\"r dO0Ag\"><a class=\"l lLrAF\" href=\"https://www.forbes.com/sites/games/2018/11/20/fallout-76-review-xbox-one-x-look-upon-my-works-and-despair/\" ping=\"/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://www.forbes.com/sites/games/2018/11/20/fallout-76-review-xbox-one-x-look-upon-my-works-and-despair/&amp;ved=0ahUKEwiyv-DTnvbhAhW2wMQHHbe2AnEQqQIIPygAMAM\">'<em>Fallout 76</em>' Review (Xbox One X): Look Upon My Works And Despair</a></h3>\n"
     ]
    }
   ],
   "source": [
    "print(\"RAW HEADLINES=\", base[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I declared an empty list called `headlines` to put the clean headlines later. To get rid of all the HTML clutter, I created a for loop that iterates over each item in the `base` list and uses the `.text` method to get rid of the HTML and give us only the text. It then appends each item to the empty headlines list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLINES = 'Fallout 76' Review (Xbox One X): Look Upon My Works And Despair\n"
     ]
    }
   ],
   "source": [
    "headlines = []\n",
    "\n",
    "for row in base:\n",
    "    clean = row.text\n",
    "    headlines.append(clean)\n",
    "    \n",
    "print(\"HEADLINES =\", headlines[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Transforming the Headlines"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In order to get the headlines ready for sentiment analysis, I have to  tokenize them. Tokenization is the process of turning a single string with multiple words in it into smaller strings each containing one word. To accomplish this, I wrote a method called `clean_tokens()` that takes a string and returns a list of individual words using the `nltk.word_tokenize()` method. I also transformed all the words into lowercase so that I don't have to worry about the difference between upper and lowercase letters when performing string functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokens(a_list):\n",
    "    words = nltk.word_tokenize(a_list)\n",
    "    clean_words = []\n",
    "    for word in words:\n",
    "        clean_words.append(word.lower())\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then used a for loop to iterate the `clean_tokens()` method from earlier over all of the headlines to create a new list of tokenized headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for each in headlines:\n",
    "    clean = clean_tokens(each)\n",
    "    tokens.append(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS = [\"'fallout\", '76', \"'\", 'review', '(', 'xbox', 'one', 'x', ')', ':', 'look', 'upon', 'my', 'works', 'and', 'despair']\n"
     ]
    }
   ],
   "source": [
    "print('TOKENS =',tokens[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the headlines are tokenized I can remove stopwords. \"Stopwords\" are common words in the english dictionary that I don't to include such as a, the, was, is etc. I imported our list of stopwords from the nltk library and then added the words \"fallout\" and \"76\" since they would show up in every search result and I didn't want to include them in the sentiment analysis. The code below is how I imported the stopwrods into our program as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append(\"fallout\")\n",
    "stopwords.append(\"76\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I have to iterate over the list of tokenized headlines and remove every instance of a stopword by comparing the tokens to the list of stopwords I just created. This is done through nested loop statements. First I declare an empty list called `filtered` to store the result. The first loop says that for each individual list of tokens within the list of headlines, create a temporary variable called x, execute the next loop and then take x and append it to `filtered`. The next loop says that for each token in the list of tokens, if it doesnt match the stopwords then append it to x. \n",
    "\n",
    "Now that sounds confusing but essentially I'm just performing three steps over and over again:\n",
    "1. Make a temporary variable\n",
    "2. Fill it with whatever words from the headlines dont match the stopwords list\n",
    "3. Append that full temporary variable to our filtered list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = []\n",
    "for list in tokens:\n",
    "    x = []\n",
    "    for word in list:\n",
    "        if word not in stopwords:\n",
    "            x.append(word)\n",
    "    filtered.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My last step was to take this list of individual tokens and put it back into a datatype that the sentiment analyzer can handle. The analyzer takes a list datatype and right now the headlines exist as a list of lists of tokens. I need to convert that back into one list of strings. To accomplish this, I wrote a loop to iterate over the filtered list and for each list of strings, join the strings together whereever there is a space. This results in a single list I called `combined` that contains a string for every headline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = []\n",
    "for list in filtered:\n",
    "    combined.append(\" \".join(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Finally Analyzing the Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have my list of clean headlines without the stopwords, I can plug them into the sentiment analyzer. First I have to import the sentiment analyzer as an object in python and declare an empty list to put the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "sia = SIA()\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I just use another for loop to iterate the `polarity_scores()` method from `sia` over each of the headlines and then append the output to the new results list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in combined:\n",
    "    pol_score = sia.polarity_scores(line)\n",
    "    results.append(pol_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it! The sentiment analyzer calculates polarity scores for each headline, creating a dictionary with the negative score, neutral score, positive score, and compound score as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.372, 'neu': 0.426, 'pos': 0.202, 'compound': -0.3818}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.247, 'neu': 0.753, 'pos': 0.0, 'compound': -0.3182}\n",
      "{'neg': 0.373, 'neu': 0.423, 'pos': 0.204, 'compound': -0.34}\n",
      "{'neg': 0.0, 'neu': 0.646, 'pos': 0.354, 'compound': 0.2944}\n",
      "{'neg': 0.0, 'neu': 0.69, 'pos': 0.31, 'compound': 0.4019}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "{'neg': 0.0, 'neu': 0.727, 'pos': 0.273, 'compound': 0.4588}\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "for d in results:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Writing the Data to CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I can write the data out to an external file to crunch it. I used the csv library to write the data out to a csv file using the code below. Essentially what its doing is iterating over the results dictionary and writing out \"compound\" and then the value associated with compound in each item in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TEST.csv', 'w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    for d in results:\n",
    "        writer.writerow(['compound', d['compound']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for reading! Hopefully this walkthrough helps you with your sentiment analyzing endeavors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
