{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fallout76 Case Study Webscraping and Sentiment Analysis Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my LIS3201 semester-long research project, my team and I investigated toxic reactions in video game journalism headlines, specifically headlines related to the video game Fallout76. We accomplished this by developing a python script that scraped Google news headlines relating to Fallout 76 and then using the Vader sentiment analyzer to conduct a sentiment analysis on the headlines we scraped. Below you will find the complete code of our first working script along with comments explaining each step of our process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to any script is to import all the necessary libraries. Here is a list of the libraries we imported and what they do:\n",
    "*requests - access webpages\n",
    "*bs4 - BeautifulSoup4, takes raw HTML from webpages and turns it into objects in python that can be manipulated easily\n",
    "*nltk - Natural Language Toolkit, leading python library for working with human language data. We used it for tokenization, the sentiment analyzer and its stopwords list (all these will be explained later).\n",
    "*csv - exporting data as Comma Seperated Value files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import nltk\n",
    "import csv\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Requesting the Webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we actually request the webpage, we need to set up a `headers` variable and a `payload` variable that we can use to request the correct webpage. The `headers` variable contains information about you as the requester so that the page knows who you are and what browser you're using. We found that unless we used this specific header, Google really did not want to cooperate with us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\":\n",
    "        \"Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `payload` variable contains information about the query you're making. In our case it included the search term, the window of time we wanted to look for results in, and the specific subset of Google News results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'as_epq': 'Fallout 76', 'tbs':'cdr:1,cd_min:11/14/2018,cd_max:12/14/2018', 'tbm':'nws'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `headers` and `payload`, we can request the webpage and save he response as an object called `r`. We used the `.get` method to go out and scrape the page. This method takes three arguments, first teh actual base url of the site you're going to, in our case Google, and then `params` which is just our `payload` variable and `headers` which is our `headers` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.google.com/search\", params=payload, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we put a print statement just to verify that the request is going to the right page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL =  https://www.google.com/search?as_epq=Fallout+76&tbs=cdr%3A1%2Ccd_min%3A11%2F14%2F2018%2Ccd_max%3A12%2F14%2F2018&tbm=nws\n"
     ]
    }
   ],
   "source": [
    "print(\"URL = \", r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Getting the Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we've requested a webpage and have it saved as the `r` object, we can start to figure out where our headlines are and pull them out of the raw HTML code. The first step here is to bring in our BeautifulSoup library to make the `r` object easier to work with. We created a new object called `soup` and then used BeautifulSoup to select only the text of r and in an lxml format so we can manipulate it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(r.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This still contains all the HTML from the website, most of which is totally irrelevant to us. So we used the `select()` method to go in and select only the elements we wanted from the whole page. `select()` takes CSS selectors as its arguments, so we put `div.g.card h3` in because all the headlines are in h3 tags within divs that have the class of g and card. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = soup.select(\"div.g.card h3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now isolated only the headlines but they still have HTML elements wrapped around them, which we need to get rid of. Below is a print statement to illustrate what one looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW HEADLINES= <h3 class=\"r dO0Ag\"><a class=\"l lLrAF\" href=\"https://www.wired.com/story/fallout-76-possible-lawsuit/\" ping=\"/url?sa=t&amp;source=web&amp;rct=j&amp;url=https://www.wired.com/story/fallout-76-possible-lawsuit/&amp;ved=0ahUKEwjY1P6tsunhAhUHBHwKHVAMBZgQqQIIPygAMAM\">Could <em>Fallout 76</em> Really Spark a Class-Action Suit?</a></h3>\n"
     ]
    }
   ],
   "source": [
    "print(\"RAW HEADLINES=\", base[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we declared an empty list called `headlines` to put our clean headlines later. To get rid of all the HTML clutter, we created a for loop that iterates over each item in the `base` list and uses the `.text` method to get rid of the HTML and give us only the text. We then appeneded each item to the empty headlines list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLINES = Could Fallout 76 Really Spark a Class-Action Suit?\n"
     ]
    }
   ],
   "source": [
    "headlines = []\n",
    "\n",
    "for row in base:\n",
    "    clean = row.text\n",
    "    headlines.append(clean)\n",
    "    \n",
    "print(\"HEADLINES =\", headlines[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Now we can start to transform these into a format that our sentiment analyzer can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Transforming the Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to take the headlines and tokenize them. Tokenization is the process of turning a single string with multiple words in it into smaller strings each containing one word. To accomplish this, we wrote a method called `clean_tokens` that takes a list of "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
